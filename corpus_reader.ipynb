{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "# DOC_PATTERN = r'.*\\.json'\n",
    "\n",
    "# corpus = PlaintextCorpusReader(f'./data/documents_json/', DOC_PATTERN)\n",
    "\n",
    "# # corpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "\n",
    "def detect_article_lang(article):\n",
    "    text = '\\n'.join(article['body_text']).split(\" \")\n",
    "    # print(text[:50])\n",
    "\n",
    "    lang = \"en\"\n",
    "    try:\n",
    "        if len(text) > 50:\n",
    "            lang = detect(\" \".join(text[:50]))\n",
    "        elif len(text) > 0:\n",
    "            lang = detect(\" \".join(text[:]))\n",
    "    # ught... beginning of the document was not in a good format\n",
    "    except Exception as e:\n",
    "        all_words = set(text)\n",
    "        try:\n",
    "            lang = detect(\" \".join(all_words))\n",
    "        # what!! :( let's see if we can find any text in abstract...\n",
    "        except Exception as e:\n",
    "            \n",
    "            try:\n",
    "                # let's try to label it through the abstract then\n",
    "                lang = detect(' '.join(article['abstract_summary']))\n",
    "            except Exception as e:\n",
    "                lang = \"unknown\"\n",
    "                pass\n",
    "    \n",
    "    return lang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breaks(content, length):\n",
    "    data = \"\"\n",
    "    words = content.split(' ')\n",
    "    total_chars = 0\n",
    "\n",
    "    # add break every length characters\n",
    "    for i in range(len(words)):\n",
    "        total_chars += len(words[i])\n",
    "        if total_chars > length:\n",
    "            data = data + \"<br>\" + words[i]\n",
    "            total_chars = 0\n",
    "        else:\n",
    "            data = data + \" \" + words[i]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import spacy\n",
    "import scispacy\n",
    "import en_core_sci_lg\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "punctuations = string.punctuation\n",
    "stopwords = list(STOP_WORDS)\n",
    "print(stopwords[:10])\n",
    "\n",
    "custom_stop_words = [\n",
    "    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n",
    "    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n",
    "    'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n",
    "]\n",
    "\n",
    "for w in custom_stop_words:\n",
    "    if w not in stopwords:\n",
    "        stopwords.append(w)\n",
    "\n",
    "# Parser\n",
    "parser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\n",
    "parser.max_length = 7000000\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n",
    "    # mytokens = \" \".join([i for i in mytokens])\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "DOC_PATTERN = r'.*\\.json'\n",
    "\n",
    "class ArticleCorpusReader(PlaintextCorpusReader, CorpusReader):\n",
    "    def __init__(self, root, fileids=DOC_PATTERN, **kwargs):\n",
    "        if any(key.startswith('metadata_path') for key in kwargs.keys()):\n",
    "            self.metadata_path = kwargs['metadata_path']\n",
    "            self.df_metadata = pd.read_csv(self.metadata_path, dtype={\n",
    "                'pubmed_id': str,\n",
    "                'Microsoft Academic Paper ID': str, \n",
    "                'doi': str\n",
    "            })\n",
    "            # print('has key')\n",
    "        # print(self.df_metadata)\n",
    "\n",
    "        PlaintextCorpusReader.__init__(self, root, fileids, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids, kwargs)\n",
    "    \n",
    "    def docs(self, fileids=None):\n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            with codecs.open(path, 'r', encoding=encoding) as file:\n",
    "                content = json.load(file)\n",
    "                dict_ = {'paper_id': None, 'title': None, 'abstract': None, 'body_text': None}\n",
    "\n",
    "                dict_['paper_id'] = content['paper_id']\n",
    "                dict_['title'] = 'Not provided.'\n",
    "                dict_['abstract'] = []\n",
    "                dict_['body_text'] = []\n",
    "                # Abstract\n",
    "                for entry in content['abstract']:\n",
    "                    dict_['abstract'].append(entry['text'])\n",
    "                # Body text\n",
    "                for entry in content['body_text']:\n",
    "                    dict_['body_text'].append(entry['text'])\n",
    "                \n",
    "                if content['metadata']['title'] != \"\":\n",
    "                    dict_['title'] = content['metadata']['title']\n",
    "\n",
    "                # dict_['abstract'] = '\\n\\n'.join(dict_['abstract'])\n",
    "                # dict_['body_text'] = '\\n\\n'.join(dict_['body_text'])\n",
    "\n",
    "                yield dict_\n",
    "\n",
    "    def metadata(self, fileids=None):\n",
    "        if fileids == None:\n",
    "            # not working\n",
    "            yield None\n",
    "        else:\n",
    "            for doc in self.docs(fileids):\n",
    "                # get metadata infomation\n",
    "                self.df_metadata = self.df_metadata.loc[self.df_metadata['sha'] == doc['paper_id']]\n",
    "\n",
    "                if len(self.df_metadata) == 0:\n",
    "                    yield None\n",
    "\n",
    "                yield self.df_metadata\n",
    "\n",
    "    def articles(self, fileids=None):\n",
    "        if fileids == None:\n",
    "            yield None\n",
    "        else:\n",
    "            for doc in self.docs(fileids):\n",
    "                dict_ = {'paper_id': None, 'doi': None, 'abstract': None, 'body_text': None, 'authors': [], 'title': None, 'journal': None, 'abstract_summary': None,\n",
    "                'abstract_word_count': 0, 'body_word_count': 0, 'body_unique_words': 0}\n",
    "\n",
    "                dict_['abstract'] = doc['abstract']\n",
    "                dict_['paper_id'] = doc['paper_id']\n",
    "                dict_['body_text'] = doc['body_text']\n",
    "\n",
    "                abstract_text = '\\n'.join(doc['abstract'])\n",
    "\n",
    "                # also create a column for the summary of abstract to be used in a plot\n",
    "                if len(doc['abstract']) == 0:\n",
    "                    # no abstract provided\n",
    "                    dict_['abstract_summary'] = [\"Not provided.\"]\n",
    "                elif len(abstract_text.split(' ')) > 100:\n",
    "                    # abstract provided is too long for plot, take first 100 words append with ...\n",
    "                    info = abstract_text.split(' ')[:100]\n",
    "                    summary = get_breaks(' '.join(info), 40)\n",
    "                    dict_['abstract_summary'] = summary + \"...\"\n",
    "                else:\n",
    "                    # abstract is short enough\n",
    "                    summary = get_breaks(abstract_text, 40)\n",
    "                    dict_['abstract_summary'] = summary\n",
    "                \n",
    "                try:\n",
    "                    # if more than one author\n",
    "                    authors = self.df_metadata['authors'].values[0].split(';')\n",
    "                    if len(authors) > 2:\n",
    "                        # if more than 2 authors, take them all with html tag breaks in between\n",
    "                        dict_['authors'].append(get_breaks('. '.join(authors), 40))\n",
    "                    else:\n",
    "                        # authors will fit in plot\n",
    "                        dict_['authors'].append(\". \".join(authors))\n",
    "                except Exception as e:\n",
    "                    # if only one author - or Null value\n",
    "                    dict_['authors'].append(self.df_metadata.loc['authors'].values[0])['title']\n",
    "\n",
    "                dict_['title'] = doc['title']\n",
    "           \n",
    "                # get word counts\n",
    "                dict_['abstract_word_count'] = len('\\n'.join(dict_['abstract']).strip().split())\n",
    "                dict_['body_word_count'] = len('\\n'.join(dict_['body_text']).strip().split())\n",
    "                dict_['body_unique_words'] = len(set('\\n'.join(dict_['body_text']).split()))\n",
    "\n",
    "                dict_['lang'] = detect_article_lang(dict_);\n",
    "\n",
    "                yield dict_       \n",
    "\n",
    "    def sizes(self, fileids=None):\n",
    "        for path in self.abspaths(fileids):\n",
    "            yield os.path.getsize(path)\n",
    "\n",
    "    def paras(self, fileids=None):\n",
    "        for article in self.articles(fileids):\n",
    "            for paragraph in article['body_text']:\n",
    "                yield paragraph\n",
    "    \n",
    "    def sents(self, fileids=None):\n",
    "        for paragraph in self.paras(fileids):\n",
    "            for sentence in sent_tokenize(paragraph):\n",
    "                yield sentence\n",
    "\n",
    "    def words(self, file2000ids=None):\n",
    "        for sentence in self.sents(fileids):\n",
    "            for word_tok in spacy_tokenizer(sentence):\n",
    "                yield word_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "article_reader = ArticleCorpusReader(f'./data/documents_json/', metadata_path=f'./data/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.df_metadata = None\n",
    "# [print(article) for article in article_reader.metadata()]\n",
    "# [str(article) for article in article_reader.docs(article_reader.fileids()[0])]\n",
    "# [str(article) for article in article_reader.metadata(article_reader.fileids()[0])]\n",
    "# [str(article) for article in article_reader.articles(article_reader.fileids()[0])]\n",
    "# [str(article) + ' kbs' for article in article_reader.sizes(article_reader.fileids()[0])]\n",
    "# [str(article) for article in article_reader.paras(article_reader.fileids()[0])]\n",
    "# [str(article) for article in article_reader.articles(article_reader.fileids()[0])]\n",
    "# [str(article) for article in article_reader.sents(article_reader.fileids()[0])]\n",
    "# [str(article) for article in article_reader.words(article_reader.fileids()[0])]\n",
    "next(article_reader.articles(article_reader.fileids()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer('Just a simple sentence here.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "class Preprocessor(object):\n",
    "    def __init__(self, corpus, target=None, **kwargs):\n",
    "        self.corpus = corpus\n",
    "        self.target = target\n",
    "\n",
    "    def fileids(self, fileids=None):\n",
    "        if fileids != None:\n",
    "            return fileids\n",
    "        return self.corpus.fileids()\n",
    "    \n",
    "    def abspath(self, fileid):\n",
    "        parent = os.path.relpath(os.path.dirname(article_reader.abspath(fileid)), article_reader.root)\n",
    "\n",
    "        basename =  os.path.basename(fileid)\n",
    "        name, ext = os.path.splitext(basename)\n",
    "\n",
    "        basename = name + '.pkl'\n",
    "\n",
    "        return os.path.normpath(os.path.join(self.target, parent, basename))\n",
    "    \n",
    "    def tokenize_article(self, article):\n",
    "        for paragraph in article['body_text']:\n",
    "            yield [ spacy_tokenizer(sent) for sent in sent_tokenize(paragraph) ]\n",
    "\n",
    "    def process(self, fileid):\n",
    "        target = self.abspath(fileid)\n",
    "        parent = os.path.dirname(target)\n",
    "\n",
    "        if not os.path.exists(parent):\n",
    "            os.makedirs(parent)\n",
    "\n",
    "        if os.path.exists(target):\n",
    "            return None\n",
    "        \n",
    "        if not os.path.isdir(parent):\n",
    "            raise ValueError(\"document path in not a directory\")\n",
    "        \n",
    "        current_article = next(self.corpus.articles(fileid))\n",
    "        # print(current_article['title'])\n",
    "        if detect_article_lang(current_article) == 'en':\n",
    "            document = { 'title': current_article['title'], 'doc': list(self.tokenize_article(current_article))}\n",
    "\n",
    "            with open(target, 'wb') as f:\n",
    "                pickle.dump(document, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            # del document\n",
    "\n",
    "            return target\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def transform(self, fileids=None):\n",
    "        if not os.path.exists(self.target):\n",
    "            os.makedirs(self.target)\n",
    "\n",
    "        for fileid in self.fileids(fileids):\n",
    "            yield self.process(fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from random import sample \n",
    "\n",
    "# instantiate preprocessor\n",
    "preprocessor_ = Preprocessor(article_reader, f'./data/pickled/')\n",
    "\n",
    "# sample files\n",
    "all_fileids = article_reader.fileids()\n",
    "sampled_fileids = sample(all_fileids, 3000)\n",
    "\n",
    "# apply preprocessor on sampled files using tqdm\n",
    "# for n in tqdm(preprocessor_.transform(sampled_fileids), total=len(sampled_fileids)):\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(article_reader.abspath('0000028b5cc154f68b8a269f6578f21e31f62977.json'))\n",
    "# next(preprocessor_.transform(['0000028b5cc154f68b8a269f6578f21e31f62977.json']))\n",
    "# os.path.relpath(os.path.dirname(article_reader.abspath(article_reader.fileids()[0])), article_reader.root)\n",
    "# article_reader.abspath('0000028b5cc154f68b8a269f6578f21e31f62977.json')\n",
    "# [str(article) for article in preprocessor_.transform(article_reader.fileids())]\n",
    "# article_reader.fileids()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "PKL_PATTERN = r'.*\\.pkl'\n",
    "\n",
    "class PickledCorpusReader(ArticleCorpusReader):\n",
    "\n",
    "    # constructor\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):        \n",
    "        # initialize upper classes\n",
    "        ArticleCorpusReader.__init__(self, root, fileids, kwargs=kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "\n",
    "    def pickled_file(self, fileids):\n",
    "        for path in self.abspaths(fileids):\n",
    "            with open(path, 'rb') as file:\n",
    "                yield pickle.load(file)\n",
    "\n",
    "    def docs(self, fileids=None):\n",
    "        # for each file in fileids open and yield handle\n",
    "        for pkl_file in self.pickled_file(fileids):\n",
    "            yield pkl_file['doc']\n",
    "    \n",
    "    # yields paragraphs\n",
    "    def paras(self, fileids=None):\n",
    "        for doc in self.docs(fileids):\n",
    "            for para in doc:\n",
    "                yield para\n",
    "\n",
    "    # yields sentences\n",
    "    def sents(self, fileids=None):\n",
    "        for para in self.paras(fileids):\n",
    "            for sent in para:\n",
    "                yield sent\n",
    "    \n",
    "    # yields words\n",
    "    def words(self, fileids=None):\n",
    "        for sent in self.sents(fileids):\n",
    "            for word in sent:\n",
    "                yield word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(f'./../knn-cosine-similarity/')\n",
    "\n",
    "import extended_similarities as sims\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from random import sample \n",
    "\n",
    "class CorpusProcessor(object):\n",
    "    def __init__(self, root, **kwargs):\n",
    "        self.corpus_fileids = None\n",
    "        self.pkl_corpus_reader = PickledCorpusReader(root)\n",
    "\n",
    "        # if sample number is chosen\n",
    "        if any(key.startswith('n_samples') for key in kwargs.keys()):\n",
    "            # initialize corpus with given size\n",
    "            n_samples = int(kwargs['n_samples'])\n",
    "            all_fileids = self.pkl_corpus_reader.fileids()\n",
    "            corpus_size = len(all_fileids)\n",
    "\n",
    "            # if given size is smaller than corpus total size\n",
    "            if n_samples <= corpus_size:\n",
    "                # sample from it\n",
    "                self.corpus_fileids = sample(all_fileids, n_samples)\n",
    "            else:\n",
    "                # else, raise error\n",
    "                raise ValueError(\"Number of samples is higher than corpus size: \" + str(corpus_size))\n",
    "\n",
    "\n",
    "    def fileids():\n",
    "        return self.pkl_corpus_reader.fileids()\n",
    "\n",
    "    def iloc(self, idx):\n",
    "        # locate doc from index in fileids array\n",
    "        return next(self.pkl_corpus_reader.pickled_file([self.corpus_fileids[idx]]))\n",
    "\n",
    "    def vectorize(self):\n",
    "        docs_text = []\n",
    "        self.vectorizer = TfidfVectorizer(max_features=2 ** 14)\n",
    "\n",
    "        # for each id in given file ids\n",
    "        for current_fileid in self.corpus_fileids:\n",
    "            # get all words in current doc\n",
    "            doc_words = self.pkl_corpus_reader.words(current_fileid)\n",
    "            # append to list of doc texts\n",
    "            docs_text.append(' '.join(doc_words))\n",
    "\n",
    "        X = self.vectorizer.fit_transform(docs_text)\n",
    "\n",
    "        # return vectorized docs\n",
    "        return X\n",
    "\n",
    "    def pca_vectors(self):\n",
    "        # initialize pca\n",
    "        self.pca = PCA(n_components=0.95, random_state=42)\n",
    "        # get vector representation of docs\n",
    "        X = self.vectorize()\n",
    "        # reduce dimensionality of given vectors\n",
    "        X_reduced = self.pca.fit_transform(X.toarray())\n",
    "        # return reduced vectors\n",
    "        return X_reduced\n",
    "\n",
    "    def knn_vectors(self):\n",
    "        # initialize knn\n",
    "        self.knn = NearestNeighbors(n_neighbors=2, algorithm='ball_tree', metric='minkowski')\n",
    "        # get reduced vectors\n",
    "        X_reduced = self.pca_vectors()\n",
    "        # get neighbors\n",
    "        neighbors = self.knn.fit(X_reduced)\n",
    "\n",
    "        return neighbors\n",
    "    \n",
    "    def cosine_knn_vectors(self):\n",
    "        # initialize cosine knn\n",
    "        self.cos_knn = sims.DistributedCosineKnn(k=3)\n",
    "        # get reduced vectors\n",
    "        X_reduced = self.pca_vectors()\n",
    "        # get neighbors\n",
    "        indices, distances = self.cos_knn.fit(input_data=X_reduced, n_bucket=7)\n",
    "\n",
    "        return (indices, distances)\n",
    "\n",
    "    def search(self, query):\n",
    "        # tokenize\n",
    "        query_tok = spacy_tokenizer(query)\n",
    "        # vectorize\n",
    "        query_x = self.vectorizer.transform(query_tok)\n",
    "        # reduce dimensionality\n",
    "        query_reduced_x = self.pca.transform(query_x.toarray())\n",
    "        # get nearest neighbors\n",
    "        distances, indices = self.knn.kneighbors(query_reduced_x, n_neighbors=5)\n",
    "\n",
    "        # show results\n",
    "        print(\"Tokens: \")\n",
    "        print(query_tok)\n",
    "        print(query_x)\n",
    "        print('\\nResults: ')\n",
    "        for idx in indices[0]:\n",
    "            print('\\t' + self.iloc(idx)['title'] + '\\n')\n",
    "\n",
    "        return distances, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_processor = CorpusProcessor(f'./data/pickled/', n_samples=6000)\n",
    "\n",
    "indices = corpus_processor.knn_vectors()\n",
    "# distances, indices = corpus_processor.cosine_knn_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_query = \"neurological impact of the disease.\"\n",
    "# search_query = \"symptoms of covid.\"\n",
    "# search_query = \"politics of the lockdown during the pandemic.\"\n",
    "search_query = \"infection vectors of the virus.\"\n",
    "\n",
    "distances, indices = corpus_processor.search(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_reduced = corpus_processor.pca_vectors()\n",
    "\n",
    "# print(X_reduced[258])\n",
    "# query_tok = spacy_tokenizer(search_query)\n",
    "# query_x = corpus_processor.vectorizer.transform(query_tok)\n",
    "# query_reduced_x = corpus_processor.pca.transform(query_x.toarray())\n",
    "# print(query_tok)\n",
    "# print(query_x)\n",
    "# print(indices)"
   ]
  }
 ]
}