{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38164bitb7e0759dcf5b42a7814cf7f27ec20b33",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = f'./data/metadata.csv'\n",
    "meta_df = pd.read_csv(metadata_path, dtype={\n",
    "    'pubmed_id': str,\n",
    "    'Microsoft Academic Paper ID': str, \n",
    "    'doi': str\n",
    "})\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jsons = glob.glob(f'./data/documents_json/*.json', recursive=True)\n",
    "len(all_jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample_size = 5000;\n",
    "sampled_json = random.sample(all_jsons, sample_size);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FileReader:\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path) as file:\n",
    "            content = json.load(file)\n",
    "            self.paper_id = content['paper_id']\n",
    "            self.abstract = []\n",
    "            self.body_text = []\n",
    "            # Abstract\n",
    "            for entry in content['abstract']:\n",
    "                self.abstract.append(entry['text'])\n",
    "            # Body text\n",
    "            for entry in content['body_text']:\n",
    "                self.body_text.append(entry['text'])\n",
    "            self.abstract = '\\n'.join(self.abstract)\n",
    "            self.body_text = '\\n'.join(self.body_text)\n",
    "    def __repr__(self):\n",
    "        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n",
    "first_row = FileReader(sampled_json[0])\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breaks(content, length):\n",
    "    data = \"\"\n",
    "    words = content.split(' ')\n",
    "    total_chars = 0\n",
    "\n",
    "    # add break every length characters\n",
    "    for i in range(len(words)):\n",
    "        total_chars += len(words[i])\n",
    "        if total_chars > length:\n",
    "            data = data + \"<br>\" + words[i]\n",
    "            total_chars = 0\n",
    "        else:\n",
    "            data = data + \" \" + words[i]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_ = {'paper_id': [], 'doi':[], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\n",
    "for idx, entry in enumerate(sampled_json):\n",
    "    if idx % (len(sampled_json) // 10) == 0:\n",
    "        print(f'Processing index: {idx} of {len(sampled_json)}')\n",
    "    \n",
    "    try:\n",
    "        content = FileReader(entry)\n",
    "    except Exception as e:\n",
    "        continue  # invalid paper format, skip\n",
    "    \n",
    "    # get metadata information\n",
    "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "    # no metadata, skip this paper\n",
    "    if len(meta_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    dict_['abstract'].append(content.abstract)\n",
    "    dict_['paper_id'].append(content.paper_id)\n",
    "    dict_['body_text'].append(content.body_text)\n",
    "    \n",
    "    # also create a column for the summary of abstract to be used in a plot\n",
    "    if len(content.abstract) == 0: \n",
    "        # no abstract provided\n",
    "        dict_['abstract_summary'].append(\"Not provided.\")\n",
    "    elif len(content.abstract.split(' ')) > 100:\n",
    "        # abstract provided is too long for plot, take first 100 words append with ...\n",
    "        info = content.abstract.split(' ')[:100]\n",
    "        summary = get_breaks(' '.join(info), 40)\n",
    "        dict_['abstract_summary'].append(summary + \"...\")\n",
    "    else:\n",
    "        # abstract is short enough\n",
    "        summary = get_breaks(content.abstract, 40)\n",
    "        dict_['abstract_summary'].append(summary)\n",
    "        \n",
    "    # get metadata information\n",
    "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "    \n",
    "    try:\n",
    "        # if more than one author\n",
    "        authors = meta_data['authors'].values[0].split(';')\n",
    "        if len(authors) > 2:\n",
    "            # if more than 2 authors, take them all with html tag breaks in between\n",
    "            dict_['authors'].append(get_breaks('. '.join(authors), 40))\n",
    "        else:\n",
    "            # authors will fit in plot\n",
    "            dict_['authors'].append(\". \".join(authors))\n",
    "    except Exception as e:\n",
    "        # if only one author - or Null value\n",
    "        dict_['authors'].append(meta_data['authors'].values[0])\n",
    "    \n",
    "    # add the title information, add breaks when needed\n",
    "    try:\n",
    "        title = get_breaks(meta_data['title'].values[0], 40)\n",
    "        dict_['title'].append(title)\n",
    "    # if title was not provided\n",
    "    except Exception as e:\n",
    "        dict_['title'].append(meta_data['title'].values[0])\n",
    "    \n",
    "    # add the journal information\n",
    "    dict_['journal'].append(meta_data['journal'].values[0])\n",
    "    \n",
    "    # add doi\n",
    "    dict_['doi'].append(meta_data['doi'].values[0])\n",
    "    \n",
    "df_covid = pd.DataFrame(dict_, columns=['paper_id', 'doi', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['abstract_word_count'] =   df_covid['abstract'].apply(lambda x: len(x.strip().split()))  # word count in abstract\n",
    "df_covid['body_word_count'] =       df_covid['body_text'].apply(lambda x: len(x.strip().split()))  # word count in body\n",
    "df_covid['body_unique_words'] =     df_covid['body_text'].apply(lambda x: len(set(str(x).split())))  # number of unique words in body\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\n",
    "df_covid['abstract'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['body_text'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.describe()"
   ]
  },
  {
   "source": [
    "# Data pre-processing\n",
    "\n",
    "df = df_covid.sample(4000, random_state=42)\n",
    "del df_covid"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "\n",
    "# set seed\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# hold label - language\n",
    "languages = []\n",
    "\n",
    "# go through each text\n",
    "for ii in tqdm(range(0, len(df))):\n",
    "    # split by space into list, take the first x index, join with space\n",
    "    text = df.iloc[ii]['body_text'].split(\" \")\n",
    "    \n",
    "    lang = \"en\"\n",
    "    try:\n",
    "        if len(text) > 50:\n",
    "            lang = detect(\" \".join(text[:50]))\n",
    "        elif len(text) > 0:\n",
    "            lang = detect(\" \".join(text[:len(text)]))\n",
    "    # ught... beginning of the document was not in a good format\n",
    "    except Exception as e:\n",
    "        all_words = set(text)\n",
    "        try:\n",
    "            lang = detect(\" \".join(all_words))\n",
    "        # what!! :( let's see if we can find any text in abstract...\n",
    "        except Exception as e:\n",
    "            \n",
    "            try:\n",
    "                # let's try to label it through the abstract then\n",
    "                lang = detect(df.iloc[ii]['abstract_summary'])\n",
    "            except Exception as e:\n",
    "                lang = \"unknown\"\n",
    "                pass\n",
    "    \n",
    "    # get the language    \n",
    "    languages.append(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "languages_dict = {}\n",
    "for lang in set(languages):\n",
    "    languages_dict[lang] = languages.count(lang)\n",
    "    \n",
    "print(\"Total: {}\\n\".format(len(languages)))\n",
    "pprint(languages_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['language'] = languages\n",
    "plt.bar(range(len(languages_dict)), list(languages_dict.values()), align='center')\n",
    "plt.xticks(range(len(languages_dict)), list(languages_dict.keys()))\n",
    "plt.title(\"Distribution of Languages in Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[df['language'] == 'en'] \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "punctuations = string.punctuation\n",
    "stopwords = list(STOP_WORDS)\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [\n",
    "    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n",
    "    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n",
    "    'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n",
    "]\n",
    "\n",
    "for w in custom_stop_words:\n",
    "    if w not in stopwords:\n",
    "        stopwords.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "import en_core_sci_lg\n",
    "\n",
    "# Parser\n",
    "parser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\n",
    "parser.max_length = 7000000\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n",
    "    mytokens = \" \".join([i for i in mytokens])\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "df[\"processed_text\"] = df[\"body_text\"].progress_apply(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(df['body_word_count'])\n",
    "df['body_word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = df['processed_text'].values\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2 ** 12)\n",
    "X = vectorizer.fit_transform(text)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_reduced= pca.fit_transform(X.toarray())\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 20\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X_reduced)\n",
    "df['y'] = y_pred\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(verbose=1, perplexity=100, random_state=42)\n",
    "X_embedded = tsne.fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.color_palette(\"bright\", 1)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n",
    "plt.title('t-SNE with no Labels')\n",
    "plt.savefig(\"t-sne_covid19.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.hls_palette(20, l=.4, s=.9)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\n",
    "plt.title('t-SNE with Kmeans Labels')\n",
    "plt.savefig(\"improved_cluster_tsne.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "model_nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree')\n",
    "neighbours = model_nbrs.fit()\n",
    "distances, indices = model_nbrs.kneighbors(pca)\n",
    "\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the COVID-19 DataFrame, too large for github\n",
    "pickle.dump(df, open(\"./pickle/df_covid.pkl\", \"wb\" ))\n",
    "\n",
    "# save the DR model\n",
    "pickle.dump(pca, open(\"./pickle/pca.pkl\", \"wb\"))\n",
    "\n",
    "# save the t-SNE model\n",
    "pickle.dump(tsne, open(\"./pickle/tsne.pkl\", \"wb\"))\n",
    "\n",
    "# save the final t-SNE\n",
    "pickle.dump(X_embedded, open(\"./pickle/X_embedded.pkl\", \"wb\" ))\n",
    "\n",
    "# save the labels generate with k-means(20)\n",
    "pickle.dump(y_pred, open(\"./pickle/y_pred.pkl\", \"wb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set test search query\n",
    "search_query = \"neurological impact of quarantine.\"\n",
    "\n",
    "# tokenize\n",
    "query_tok = spacy_tokenizer(search_query)\n",
    "# vectorize\n",
    "query_x = vectorizer.transform([query_tok])\n",
    "# reduce dimentionality\n",
    "query_reduced_x = pca.transform(query_x.toarray());\n",
    "\n",
    "# find neighbours\n",
    "\n",
    "# plot in tsne chart\n",
    "# query_embedded = tsne.transform(query_reduced_x.toarray())\n",
    "\n",
    "print(query_embedded)"
   ]
  }
 ]
}